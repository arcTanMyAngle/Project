{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10705,"sourceType":"datasetVersion","datasetId":7160},{"sourceId":2243895,"sourceType":"datasetVersion","datasetId":1347338},{"sourceId":9715737,"sourceType":"datasetVersion","datasetId":5943650}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install required libraries\n!pip install --quiet pdf2image\n!apt-get install -y poppler-utils\n!pip install --quiet fpdf\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:05:49.321772Z","iopub.execute_input":"2024-10-25T22:05:49.322235Z","iopub.status.idle":"2024-10-25T22:06:25.240379Z","shell.execute_reply.started":"2024-10-25T22:05:49.322196Z","shell.execute_reply":"2024-10-25T22:06:25.239233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pdf2image import convert_from_path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Lambda, Concatenate\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:16:31.319758Z","iopub.execute_input":"2024-10-25T22:16:31.320182Z","iopub.status.idle":"2024-10-25T22:16:31.326877Z","shell.execute_reply.started":"2024-10-25T22:16:31.320137Z","shell.execute_reply":"2024-10-25T22:16:31.325836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Load and preprocess data\ndef load_data():\n    # Load training data\n    train_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv')\n    X_train = train_data.iloc[:, 1:].values\n    y_train = train_data.iloc[:, 0].values\n    \n    # Load test data\n    test_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-test.csv')\n    X_test = test_data.iloc[:, 1:].values\n    y_test = test_data.iloc[:, 0].values\n    \n    # Reshape images to 28x28x1\n    X_train = X_train.reshape(-1, 28, 28, 1)\n    X_test = X_test.reshape(-1, 28, 28, 1)\n    \n    # Normalize pixel values\n    X_train = X_train.astype('float32') / 255.0\n    X_test = X_test.astype('float32') / 255.0\n    \n    return X_train, y_train, X_test, y_test\n\n# Create the forensic analysis model\ndef create_model():\n    input_shape = (28, 28, 1)\n    \n    # Input layer\n    inputs = Input(shape=input_shape)\n    \n    # First convolutional block - Feature extraction\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    # Second convolutional block - Stroke analysis\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    # Third convolutional block - Deep features\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    # Flatten and dense layers\n    x = Flatten()(x)\n    x = Dense(512, activation='relu', name='forensic_dense')(x)\n    x = Dropout(0.5)(x)\n    \n    # Output layer - number of classes (26 for letters)\n    outputs = Dense(26, activation='softmax')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Function to extract forensic features\ndef extract_forensic_features(model, image_batch):\n    \"\"\"Extract intermediate features for forensic analysis\"\"\"\n    feature_model = Model(inputs=model.input, outputs=model.get_layer('forensic_dense').output)\n    features = feature_model.predict(image_batch)\n    return features\n\n# Function to compare two characters\ndef compare_characters(model, char1, char2):\n    \"\"\"Compare two characters and return similarity score\"\"\"\n    # Extract features\n    feat1 = extract_forensic_features(model, char1.reshape(1, 28, 28, 1))\n    feat2 = extract_forensic_features(model, char2.reshape(1, 28, 28, 1))\n    \n    # Compute cosine similarity\n    similarity = np.dot(feat1, feat2.T) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))\n    return similarity[0][0]\n\n# Main training and evaluation function\ndef main():\n    print(\"Loading data...\")\n    X_train, y_train, X_test, y_test = load_data()\n    \n    # Adjust labels to 0-based indexing\n    y_train = y_train - 1\n    y_test = y_test - 1\n    \n    print(\"Creating model...\")\n    model = create_model()\n    \n    # Compile model\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n    \n    print(\"Training model...\")\n    # Train with early stopping for efficiency\n    history = model.fit(X_train, y_train,\n                       batch_size=128,\n                       epochs=10,  # Reduced epochs for quick training\n                       validation_split=0.1,\n                       verbose=1)\n    \n    print(\"\\nEvaluating model...\")\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"Test accuracy: {test_acc:.4f}\")\n    \n    # Demonstrate forensic comparison\n    print(\"\\nDemonstrating character comparison...\")\n    # Compare two random test characters\n    idx1, idx2 = np.random.randint(0, len(X_test), 2)\n    similarity = compare_characters(model, X_test[idx1], X_test[idx2])\n    print(f\"Similarity score between test characters: {similarity:.4f}\")\n    print(f\"True labels: {chr(65 + y_test[idx1])} and {chr(65 + y_test[idx2])}\")\n    \n    return model, history\n\n# Additional utility functions for forensic analysis\ndef analyze_character_features(model, character):\n    \"\"\"Detailed analysis of a single character\"\"\"\n    features = extract_forensic_features(model, character.reshape(1, 28, 28, 1))\n    \n    # Basic statistical analysis\n    feature_stats = {\n        'mean': np.mean(features),\n        'std': np.std(features),\n        'max': np.max(features),\n        'min': np.min(features)\n    }\n    \n    return feature_stats\n\ndef batch_compare_characters(model, reference_char, comparison_chars):\n    \"\"\"Compare one character against multiple others\"\"\"\n    similarities = []\n    for char in comparison_chars:\n        sim = compare_characters(model, reference_char, char)\n        similarities.append(sim)\n    return np.array(similarities)\n\n# Run the main function\nif __name__ == \"__main__\":\n    print(\"Starting forensic handwriting analysis system...\")\n    model, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:33:14.527925Z","iopub.execute_input":"2024-10-25T23:33:14.528342Z","iopub.status.idle":"2024-10-25T23:34:36.436058Z","shell.execute_reply.started":"2024-10-25T23:33:14.528304Z","shell.execute_reply":"2024-10-25T23:34:36.434675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This will run the complete training and analysis pipeline\nmodel, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:35:49.238375Z","iopub.execute_input":"2024-10-25T23:35:49.238754Z","iopub.status.idle":"2024-10-25T23:37:11.037417Z","shell.execute_reply.started":"2024-10-25T23:35:49.238720Z","shell.execute_reply":"2024-10-25T23:37:11.036408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_history(history):\n    # Plot training & validation accuracy values\n    plt.figure(figsize=(12, 4))\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    plt.show()\n\n# Assuming `history` is the output from model.fit\nplot_training_history(history)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:54:14.497256Z","iopub.execute_input":"2024-10-25T19:54:14.498066Z","iopub.status.idle":"2024-10-25T19:54:15.018718Z","shell.execute_reply.started":"2024-10-25T19:54:14.498023Z","shell.execute_reply":"2024-10-25T19:54:15.017679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_features(features):\n    plt.figure(figsize=(12, 6))\n    plt.plot(features.flatten(), label=\"Feature Intensity\")\n    plt.xlabel(\"Feature Index\")\n    plt.ylabel(\"Intensity\")\n    plt.title(\"Intermediate Forensic Features\")\n    plt.legend()\n    plt.show()\n# Load the data if not already loaded\nX_train, y_train, X_test, y_test = load_data()\n\n# Visualize features of a single character (example with the first test character)\nfeatures = extract_forensic_features(model, X_test[0].reshape(1, 28, 28, 1))\nvisualize_features(features)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:57:22.681191Z","iopub.execute_input":"2024-10-25T19:57:22.682004Z","iopub.status.idle":"2024-10-25T19:57:30.378325Z","shell.execute_reply.started":"2024-10-25T19:57:22.681962Z","shell.execute_reply":"2024-10-25T19:57:30.377439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_comparison(X_test, y_test, idx1, idx2, similarity_score):\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n    \n    # Show first character\n    axes[0].imshow(X_test[idx1].reshape(28, 28), cmap='gray')\n    axes[0].set_title(f\"Label: {chr(65 + y_test[idx1])}\")\n    \n    # Show second character\n    axes[1].imshow(X_test[idx2].reshape(28, 28), cmap='gray')\n    axes[1].set_title(f\"Label: {chr(65 + y_test[idx2])}\")\n    \n    plt.suptitle(f\"Similarity Score: {similarity_score:.4f}\", fontsize=16)\n    plt.show()\n\n# Example usage\nidx1, idx2 = np.random.randint(0, len(X_test), 2)\nsimilarity_score = compare_characters(model, X_test[idx1], X_test[idx2])\nvisualize_comparison(X_test, y_test, idx1, idx2, similarity_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:59:43.892825Z","iopub.execute_input":"2024-10-25T19:59:43.893608Z","iopub.status.idle":"2024-10-25T19:59:44.768546Z","shell.execute_reply.started":"2024-10-25T19:59:43.893567Z","shell.execute_reply":"2024-10-25T19:59:44.767598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Lambda, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\nclass ForensicCharacterAnalyzer:\n    def __init__(self):\n        self.model = self.create_model()\n        \n    def create_model(self):\n        \"\"\"Create the forensic analysis model\"\"\"\n        input_shape = (28, 28, 1)\n        \n        # Input layer\n        inputs = Input(shape=input_shape)\n        \n        # First convolutional block - Feature extraction\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n        \n        # Second convolutional block - Deeper features\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n        \n        # Third convolutional block - Fine details\n        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n        \n        # Flatten and dense layers\n        x = Flatten()(x)\n        x = Dense(512, activation='relu', name='forensic_features')(x)\n        x = Dropout(0.5)(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.5)(x)\n        \n        # Output layer for writer classification\n        outputs = Dense(6, activation='softmax', name='writer_output')(x)  # 6 writers\n        \n        model = Model(inputs=inputs, outputs=outputs)\n        return model\n    \n    def extract_features(self, image_batch):\n        \"\"\"Extract intermediate features for forensic analysis\"\"\"\n        feature_model = Model(\n            inputs=self.model.input,\n            outputs=self.model.get_layer('forensic_features').output\n        )\n        features = feature_model.predict(image_batch)\n        return features\n    \n    def compare_samples(self, sample1, sample2):\n        \"\"\"Compare two handwriting samples and return similarity score\"\"\"\n        # Extract features\n        feat1 = self.extract_features(sample1)\n        feat2 = self.extract_features(sample2)\n        \n        # Compute cosine similarity\n        similarity = np.dot(feat1.flatten(), feat2.flatten()) / (\n            np.linalg.norm(feat1.flatten()) * np.linalg.norm(feat2.flatten())\n        )\n        return similarity\n\ndef load_and_preprocess_data():\n    \"\"\"Load and preprocess the handwriting data\"\"\"\n    # Base paths\n    base_path = '/kaggle/input/personalprofile'\n    \n    # Training participants (Macey and Mia)\n    training_participants = ['Macey', 'Mia']\n    X_train = []\n    y_train = []\n    \n    for name in training_participants:\n        participant_path = os.path.join(base_path, name)\n        for file in os.listdir(participant_path):\n            if file.endswith('.pdf'):\n                pdf_path = os.path.join(participant_path, file)\n                images = convert_from_path(pdf_path)\n                for img in images:\n                    # Convert to grayscale and resize\n                    img_gray = img.convert('L')\n                    img_resized = img_gray.resize((28, 28))\n                    # Convert to array and normalize\n                    img_array = img_to_array(img_resized)\n                    img_array = img_array.astype('float32') / 255.0\n                    X_train.append(img_array)\n                    y_train.append(name)\n    \n    # Convert to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    \n    # Encode labels\n    le = LabelEncoder()\n    all_participants = ['Emmanuel', 'Macey', 'Mia', 'Tam', 'Victoria', 'Suyesh']\n    le.fit(all_participants)\n    y_train = le.transform(y_train)\n    \n    return X_train, y_train, le\n\ndef main():\n    \"\"\"Main training and analysis function\"\"\"\n    try:\n        # Create analyzer\n        analyzer = ForensicCharacterAnalyzer()\n        \n        # Load and preprocess data\n        print(\"Loading data...\")\n        X_train, y_train, le = load_and_preprocess_data()\n        \n        # Compile model\n        analyzer.model.compile(\n            optimizer=Adam(learning_rate=0.001),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Train model\n        print(\"Training model...\")\n        history = analyzer.model.fit(\n            X_train, y_train,\n            batch_size=32,\n            epochs=10,\n            validation_split=0.2,\n            verbose=1\n        )\n        \n        return analyzer, history\n        \n    except Exception as e:\n        print(f\"Error in main function: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    analyzer, history = main()\n    \n    # Plot training results\n    plt.figure(figsize=(12, 4))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training')\n    plt.plot(history.history['val_accuracy'], label='Validation')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training')\n    plt.plot(history.history['val_loss'], label='Validation')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:17:17.751717Z","iopub.execute_input":"2024-10-25T22:17:17.752443Z","iopub.status.idle":"2024-10-25T22:17:33.988440Z","shell.execute_reply.started":"2024-10-25T22:17:17.752391Z","shell.execute_reply":"2024-10-25T22:17:33.987429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import LabelEncoder\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import img_to_array","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:36:21.475762Z","iopub.execute_input":"2024-10-25T22:36:21.476164Z","iopub.status.idle":"2024-10-25T22:36:21.482441Z","shell.execute_reply.started":"2024-10-25T22:36:21.476127Z","shell.execute_reply":"2024-10-25T22:36:21.481578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First recreate your original model\nclass ForensicCharacterAnalyzer:\n    def __init__(self):\n        self.model = self.create_model()\n        \n    def create_model(self):\n        \"\"\"Create the forensic analysis model\"\"\"\n        input_shape = (28, 28, 1)\n        \n        # Input layer\n        inputs = Input(shape=input_shape)\n        \n        # First convolutional block - Feature extraction\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n        \n        # Second convolutional block - Stroke analysis\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n        \n        # Third convolutional block - Deep features\n        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n        \n        # Flatten and dense layers\n        x = Flatten()(x)\n        x = Dense(512, activation='relu', name='forensic_dense')(x)\n        x = Dropout(0.5)(x)\n        \n        # Output layer for letters (26 classes)\n        outputs = Dense(26, activation='softmax')(x)\n        \n        model = Model(inputs=inputs, outputs=outputs)\n        return model\n\n# Now create the participant classifier\ndef create_participant_classifier(base_model):\n    try:\n        # Get the output of the forensic_dense layer from base model\n        feature_extractor = Model(\n            inputs=base_model.model.input,  # Note the change here\n            outputs=base_model.model.get_layer('forensic_dense').output  # And here\n        )\n        \n        # Freeze the base model layers\n        for layer in feature_extractor.layers:\n            layer.trainable = False\n        \n        # Create new model for participant classification\n        inputs = Input(shape=(28, 28, 1))\n        x = feature_extractor(inputs)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.5)(x)\n        outputs = Dense(6, activation='softmax')(x)  # 6 participants\n        \n        participant_model = Model(inputs=inputs, outputs=outputs)\n        return participant_model\n        \n    except Exception as e:\n        print(f\"Error creating participant classifier: {str(e)}\")\n        raise\n\ndef main():\n    try:\n        # First create and train the EMNIST model\n        print(\"Creating base analyzer...\")\n        base_analyzer = ForensicCharacterAnalyzer()\n        \n        # Load and preprocess EMNIST data\n        print(\"Loading EMNIST data...\")\n        X_train, y_train, X_test, y_test = load_data()  # Your original load_data function\n        \n        # Train the base model\n        print(\"Training base model...\")\n        base_analyzer.model.compile(\n            optimizer=Adam(learning_rate=0.001),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        base_history = base_analyzer.model.fit(\n            X_train, y_train,\n            batch_size=128,\n            epochs=10,\n            validation_split=0.1,\n            verbose=1\n        )\n        \n        # Now train the participant classifier\n        print(\"\\nTraining participant classifier...\")\n        participant_model, history, le = train_participant_classifier(base_analyzer)\n        \n        print(\"\\nEvaluating classifier...\")\n        y_true, y_pred = evaluate_participant_classifier(participant_model, le)\n        \n        # Print classification report\n        from sklearn.metrics import classification_report\n        print(\"\\nClassification Report:\")\n        print(classification_report(y_true, y_pred))\n        \n        return base_analyzer, participant_model, history\n        \n    except Exception as e:\n        print(f\"Error in main execution: {str(e)}\")\n        raise\n\n# Modified train_participant_classifier to accept the analyzer\ndef train_participant_classifier(base_analyzer):\n    try:\n        # Load participant data\n        print(\"Loading participant data...\")\n        X_train, y_train, le = load_and_preprocess_data()\n        print(f\"Total samples: {len(X_train)}\")\n        \n        # Create and compile participant classifier\n        print(\"Creating participant classifier...\")\n        participant_model = create_participant_classifier(base_analyzer)\n        participant_model.compile(\n            optimizer=Adam(learning_rate=0.0001),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Train the model\n        print(\"Training participant classifier...\")\n        history = participant_model.fit(\n            X_train, y_train,\n            batch_size=8,\n            epochs=20,\n            validation_split=0.2,\n            callbacks=[\n                tf.keras.callbacks.EarlyStopping(\n                    monitor='val_loss',\n                    patience=5,\n                    restore_best_weights=True\n                )\n            ],\n            verbose=1\n        )\n        \n        return participant_model, history, le\n        \n    except Exception as e:\n        print(f\"Error training participant classifier: {str(e)}\")\n        raise\n\n# Run everything\nif __name__ == \"__main__\":\n    base_analyzer, participant_model, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:41:07.583377Z","iopub.execute_input":"2024-10-25T22:41:07.583827Z","iopub.status.idle":"2024-10-25T22:43:02.445539Z","shell.execute_reply.started":"2024-10-25T22:41:07.583785Z","shell.execute_reply":"2024-10-25T22:43:02.444572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pdf2image import convert_from_path\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef extract_images_from_pdf(pdf_path, output_dir=None):\n    \"\"\"Extract images from PDF and save them temporarily if output_dir is provided\"\"\"\n    try:\n        # Convert PDF to images\n        images = convert_from_path(pdf_path)\n        \n        if output_dir:\n            os.makedirs(output_dir, exist_ok=True)\n            saved_paths = []\n            for i, image in enumerate(images):\n                image_path = os.path.join(output_dir, f'page_{i+1}.png')\n                image.save(image_path, 'PNG')\n                saved_paths.append(image_path)\n            return saved_paths\n        return images\n    except Exception as e:\n        print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n        return []\n\ndef process_writer_pdfs(base_dir):\n    \"\"\"Process all PDFs in the personalprofile directory\"\"\"\n    writer_samples = {}\n    \n    # Process each writer's directory\n    for writer_name in os.listdir(base_dir):\n        writer_path = os.path.join(base_dir, writer_name)\n        if os.path.isdir(writer_path):\n            writer_samples[writer_name] = []\n            \n            # Process each file in writer's directory\n            for file_name in os.listdir(writer_path):\n                if file_name.lower().endswith('.pdf'):\n                    pdf_path = os.path.join(writer_path, file_name)\n                    images = extract_images_from_pdf(pdf_path)\n                    \n                    # Convert PIL images to format needed for model\n                    for img in images:\n                        # Convert to grayscale and numpy array\n                        img_gray = img.convert('L')\n                        img_array = np.array(img_gray)\n                        \n                        # Resize to 28x28 (model's expected input size)\n                        img_resized = cv2.resize(img_array, (28, 28))\n                        \n                        # Normalize and reshape\n                        img_processed = img_resized.astype('float32') / 255.0\n                        img_processed = img_processed.reshape(1, 28, 28, 1)\n                        \n                        writer_samples[writer_name].append(img_processed)\n    \n    return writer_samples\n\ndef compare_writers(model, writer_samples):\n    \"\"\"Compare handwriting samples between writers\"\"\"\n    results = {}\n    \n    for writer1 in writer_samples:\n        results[writer1] = {}\n        for writer2 in writer_samples:\n            if writer1 != writer2:\n                similarities = []\n                \n                # Compare each sample from writer1 with each sample from writer2\n                for sample1 in writer_samples[writer1]:\n                    for sample2 in writer_samples[writer2]:\n                        similarity = compare_characters(model, sample1[0], sample2[0])\n                        similarities.append(similarity)\n                \n                # Calculate average similarity\n                avg_similarity = np.mean(similarities) if similarities else 0\n                results[writer1][writer2] = avg_similarity\n    \n    return results\n\ndef visualize_comparison_results(results):\n    \"\"\"Visualize the comparison results as a heatmap\"\"\"\n    writers = list(results.keys())\n    similarity_matrix = np.zeros((len(writers), len(writers)))\n    \n    for i, writer1 in enumerate(writers):\n        for j, writer2 in enumerate(writers):\n            if writer1 != writer2:\n                similarity_matrix[i, j] = results[writer1][writer2]\n    \n    plt.figure(figsize=(10, 8))\n    plt.imshow(similarity_matrix, cmap='Blues')\n    plt.colorbar(label='Similarity Score')\n    plt.xticks(range(len(writers)), writers, rotation=45)\n    plt.yticks(range(len(writers)), writers)\n    plt.title('Handwriting Similarity Matrix')\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\nbase_dir = \"/kaggle/input/personalprofile\"\nwriter_samples = process_writer_pdfs(base_dir)\ncomparison_results = compare_writers(model, writer_samples)\nvisualize_comparison_results(comparison_results)\n\n# Print detailed results\nprint(\"\\nDetailed Comparison Results:\")\nfor writer1, comparisons in comparison_results.items():\n    print(f\"\\n{writer1}'s writing compared to:\")\n    for writer2, similarity in comparisons.items():\n        print(f\"  {writer2}: {similarity:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:08:22.685192Z","iopub.execute_input":"2024-10-25T23:08:22.685717Z","iopub.status.idle":"2024-10-25T23:11:05.686005Z","shell.execute_reply.started":"2024-10-25T23:08:22.685678Z","shell.execute_reply":"2024-10-25T23:11:05.685065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, Lambda\nfrom tensorflow.keras.optimizers import Adam\nimport cv2\nfrom scipy.stats import skew, kurtosis\n\nclass ForensicCharacterAnalyzer:\n    def __init__(self):\n        self.model = None\n        \n    def load_data(self):\n        \"\"\"Load and preprocess EMNIST dataset with enhanced normalization\"\"\"\n        print(\"Loading data...\")\n        train_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv')\n        test_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-test.csv')\n        \n        X_train = train_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n        y_train = train_data.iloc[:, 0].values - 1  # Adjust to 0-based indexing\n        \n        X_test = test_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n        y_test = test_data.iloc[:, 0].values - 1\n        \n        return X_train, y_train, X_test, y_test\n    \n    def create_model(self):\n        \"\"\"Create enhanced model with forensic-specific layers\"\"\"\n        input_shape = (28, 28, 1)\n        inputs = Input(shape=input_shape)\n\n        # Stroke Analysis Branch\n        def create_stroke_features(x):\n            # Sobel filters for edge detection\n            sobel = tf.image.sobel_edges(x)  # Shape: (batch, height, width, channels, 2)\n            sobel_x = sobel[..., 0]  # Get x component\n            sobel_y = sobel[..., 1]  # Get y component\n            \n            # Calculate stroke angles and magnitudes\n            angles = tf.math.atan2(sobel_y, sobel_x)  # Shape: (batch, height, width, channels)\n            magnitude = tf.sqrt(tf.square(sobel_x) + tf.square(sobel_y))\n            \n            # Remove the extra channel dimension and concatenate\n            angles = tf.squeeze(angles, axis=-1)\n            magnitude = tf.squeeze(magnitude, axis=-1)\n            \n            # Add feature channel dimension\n            angles = tf.expand_dims(angles, axis=-1)\n            magnitude = tf.expand_dims(magnitude, axis=-1)\n            \n            # Concatenate along the feature dimension\n            return tf.concat([angles, magnitude], axis=-1)  # Shape: (batch, height, width, 2)\n\n        # Initial convolution layers - Main branch\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        # Process stroke features in parallel - Stroke branch\n        stroke_features = Lambda(create_stroke_features)(inputs)  # Shape: (batch, 28, 28, 2)\n        stroke_features = Conv2D(16, (3, 3), activation='relu', padding='same')(stroke_features)\n        stroke_features = MaxPooling2D((2, 2))(stroke_features)  # Shape: (batch, 14, 14, 16)\n\n        # Concatenate features from both branches\n        # After MaxPooling, x shape is (batch, 14, 14, 32)\n        # After MaxPooling, stroke_features shape is (batch, 14, 14, 16)\n        x = tf.concat([x, stroke_features], axis=-1)  # Result shape: (batch, 14, 14, 48)\n\n        # Deeper feature extraction\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(512, activation='relu', name='forensic_features')(x)\n        x = Dropout(0.5)(x)\n\n        outputs = Dense(26, activation='softmax')(x)\n\n        self.model = Model(inputs=inputs, outputs=outputs)\n        return self.model\n\n    def analyze_stroke_characteristics(self, image):\n        \"\"\"Analyze stroke characteristics including pressure, angles, and consistency\"\"\"\n        # Convert to numpy array if needed\n        if isinstance(image, tf.Tensor):\n            image = image.numpy()\n        \n        # Ensure proper shape\n        if len(image.shape) == 4:\n            image = image[0]\n        if len(image.shape) == 3:\n            image = image[:,:,0]\n        \n        # Convert to uint8 for OpenCV processing\n        img_uint8 = (image * 255).astype(np.uint8)\n        \n        # Edge detection for stroke analysis\n        edges = cv2.Canny(img_uint8, 50, 150)\n        \n        # Calculate gradients for pressure analysis\n        sobelx = cv2.Sobel(img_uint8, cv2.CV_64F, 1, 0, ksize=3)\n        sobely = cv2.Sobel(img_uint8, cv2.CV_64F, 0, 1, ksize=3)\n        \n        # Stroke angle analysis\n        angles = np.arctan2(sobely, sobelx)\n        magnitudes = np.sqrt(sobelx**2 + sobely**2)\n        \n        # Calculate stroke characteristics\n        stroke_analysis = {\n            'pressure_mean': np.mean(magnitudes),\n            'pressure_std': np.std(magnitudes),\n            'angle_mean': np.mean(angles),\n            'angle_std': np.std(angles),\n            'stroke_consistency': 1.0 / (np.std(magnitudes) + 1e-6),\n            'edge_density': np.sum(edges > 0) / edges.size,\n            'pressure_skew': skew(magnitudes.flatten()),\n            'angle_distribution': np.histogram(angles.flatten(), bins=8)[0].tolist()\n        }\n        \n        return stroke_analysis\n\n    def extract_forensic_features(self, char_image):\n        \"\"\"Extract deep learning features for forensic analysis\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model must be trained before extracting features\")\n            \n        feature_model = Model(inputs=self.model.input,\n                            outputs=self.model.get_layer('forensic_features').output)\n        features = feature_model.predict(char_image.reshape(1, 28, 28, 1))\n        \n        # Calculate statistical measures\n        feature_stats = {\n            'mean': np.mean(features),\n            'std': np.std(features),\n            'skewness': skew(features.flatten()),\n            'kurtosis': kurtosis(features.flatten()),\n            'quartiles': np.percentile(features, [25, 50, 75]).tolist(),\n            'feature_vector': features.flatten()\n        }\n        \n        return feature_stats\n    \n    def compare_characters(self, char1, char2):\n        \"\"\"Enhanced character comparison with multiple similarity metrics\"\"\"\n        # Get both feature types\n        feat1_deep = self.extract_forensic_features(char1)\n        feat2_deep = self.extract_forensic_features(char2)\n        \n        stroke1 = self.analyze_stroke_characteristics(char1)\n        stroke2 = self.analyze_stroke_characteristics(char2)\n        \n        # Calculate multiple similarity metrics\n        comparison = {\n            'feature_similarity': self._cosine_similarity(\n                feat1_deep['feature_vector'],\n                feat2_deep['feature_vector']\n            ),\n            'pressure_similarity': 1 - abs(\n                stroke1['pressure_mean'] - stroke2['pressure_mean']\n            ) / max(stroke1['pressure_mean'], stroke2['pressure_mean']),\n            'stroke_consistency_similarity': 1 - abs(\n                stroke1['stroke_consistency'] - stroke2['stroke_consistency']\n            ) / max(stroke1['stroke_consistency'], stroke2['stroke_consistency']),\n            'angle_similarity': self._compare_angle_distributions(\n                stroke1['angle_distribution'],\n                stroke2['angle_distribution']\n            )\n        }\n        \n        # Calculate weighted overall similarity\n        comparison['overall_similarity'] = (\n            0.4 * comparison['feature_similarity'] +\n            0.3 * comparison['pressure_similarity'] +\n            0.2 * comparison['stroke_consistency_similarity'] +\n            0.1 * comparison['angle_similarity']\n        )\n        \n        return comparison\n    \n    def _cosine_similarity(self, v1, v2):\n        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    \n    def _compare_angle_distributions(self, dist1, dist2):\n        \"\"\"Compare angle distributions using Jensen-Shannon divergence\"\"\"\n        dist1 = np.array(dist1) + 1e-10  # Avoid zero division\n        dist2 = np.array(dist2) + 1e-10\n        dist1 = dist1 / np.sum(dist1)\n        dist2 = dist2 / np.sum(dist2)\n        m = 0.5 * (dist1 + dist2)\n        js_div = 0.5 * (np.sum(dist1 * np.log(dist1 / m)) + np.sum(dist2 * np.log(dist2 / m)))\n        return 1 / (1 + js_div)  # Convert to similarity score\n\ndef main():\n    # Initialize analyzer\n    analyzer = ForensicCharacterAnalyzer()\n    \n    # Load data\n    X_train, y_train, X_test, y_test = analyzer.load_data()\n    \n    # Create and compile model\n    model = analyzer.create_model()\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n    \n    # Train model\n    print(\"Training model...\")\n    history = model.fit(X_train, y_train,\n                       batch_size=128,\n                       epochs=10,\n                       validation_split=0.1,\n                       verbose=1)\n    \n    # Example analysis\n    print(\"\\nPerforming forensic analysis on sample characters...\")\n    idx1, idx2 = np.random.randint(0, len(X_test), 2)\n    comparison = analyzer.compare_characters(X_test[idx1], X_test[idx2])\n    \n    print(\"\\nForensic Comparison Results:\")\n    for metric, value in comparison.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    return analyzer, history\n\nif __name__ == \"__main__\":\n    analyzer, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T20:16:48.758758Z","iopub.execute_input":"2024-10-25T20:16:48.759155Z","iopub.status.idle":"2024-10-25T20:16:56.550135Z","shell.execute_reply.started":"2024-10-25T20:16:48.759117Z","shell.execute_reply":"2024-10-25T20:16:56.548718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}